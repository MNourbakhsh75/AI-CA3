{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - Computer Assignment 3 - Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehrdad Nourbakhsh(810194418)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we want to use Bayes rule for classifying news based on a short description of that news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with nearly 23000 news. each news has a category. we want to create a model to classify the category of that news based on the short description of that news. we read the dataset from .csv file and load that into a panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Påskekrim is merely the tip of the proverbial ...  \n",
       "1                                                NaN  \n",
       "2  Madonna is slinking her way into footwear now,...  \n",
       "3  But what if you're a 30-something couple that ...  \n",
       "4  Obamacare was supposed to make birth control f...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_csv('Attachment/data.csv')\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each news has some other information too rather than the short description (e.g headline, authors). but we ignore them and use only words of the short description to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should preprocess the data and use the result for the model. we want to normalize the short description.\n",
    "we use nltk library for normalizing the text.normalization include these steps:\n",
    "* converting uppercase to lowercase\n",
    "* removing punctuation signs\n",
    "* removing numbers\n",
    "* converting each text into a list of words\n",
    "* removing stop words\n",
    "* using lemmatization to remove inflectional endings only and to return the base form of a word\n",
    "\n",
    "If we don't convert all words to lowercase, our model might treat a word which is at the beginning of a sentence with a capital letter, different from the same word which appears later in the sentence but without any capital latter. this might influence our model accuracy. thus we convert all letters to lowercase.\n",
    "\n",
    "In our model, we use words frequency and occurrences of them in the text. we want to find relevant results not only for the exact expression but also for the other possible forms of the words we used. for this purpose we use lemmatization. lemmatization helps us to treat all possible forms of a word as an individual word and this can improve our model with increasing word frequency for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>[påskekrim, merely, tip, proverbial, iceberg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>[madonna, slink, way, footwear, truth, dare, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>[something, couple, shy, away, table, dance, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>[obamacare, suppose, make, birth, control, fre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Påskekrim is merely the tip of the proverbial ...   \n",
       "1                                                NaN   \n",
       "2  Madonna is slinking her way into footwear now,...   \n",
       "3  But what if you're a 30-something couple that ...   \n",
       "4  Obamacare was supposed to make birth control f...   \n",
       "\n",
       "                                               words  \n",
       "0  [påskekrim, merely, tip, proverbial, iceberg, ...  \n",
       "1                                                 []  \n",
       "2  [madonna, slink, way, footwear, truth, dare, p...  \n",
       "3  [something, couple, shy, away, table, dance, e...  \n",
       "4  [obamacare, suppose, make, birth, control, fre...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanText(df):\n",
    "    \n",
    "    data = df.short_description\n",
    "    data = data.fillna('')\n",
    "    data = data.str.lower()\n",
    "    data = data.str.replace('[^\\w\\s]',' ')\n",
    "    data = data.str.replace('\\d+', '')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordsList = data.apply(word_tokenize)\n",
    "    wordsList = wordsList.apply(lambda x: [item for item in x if item not in stopWords])\n",
    "    wordsList = wordsList.apply(lambda x: [lemmatizer.lemmatize(y, pos=\"v\") for y in x])\n",
    "    df['words'] = wordsList\n",
    "    return df\n",
    "\n",
    "dataFrame = cleanText(dataFrame)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to separate our data into two parts. the train set and evaluate set. we use 80 percent of data as train set and 20 percent as evaluate set. we want to have all sorts of news. if we choose 80 percent of real data as train set, we may have a set with only one or two categories, therefore, our model can not detect other categories as well. we have to create a train set with good diversity from all of the categories. \n",
    "\n",
    "for each category, we choose a random subset of that category (80 percent) and then combine those subsets to create our train set and the remaining 20 percent of each category for evaluate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel = dataFrame[dataFrame['category'] == 'TRAVEL']\n",
    "business = dataFrame[dataFrame['category'] == 'BUSINESS']\n",
    "sb = dataFrame[dataFrame['category'] == 'STYLE & BEAUTY']\n",
    "trainTravel = travel.sample(frac=0.8)\n",
    "evaluateTravel = travel.drop(trainTravel.index)\n",
    "trainBusiness = business.sample(frac=0.8)\n",
    "evaluateBusiness = business.drop(trainBusiness.index)\n",
    "trainSB = sb.sample(frac=0.8)\n",
    "evaluateSB = sb.drop(trainSB.index)\n",
    "evaluateData = pd.concat([evaluateSB,evaluateBusiness, evaluateTravel])\n",
    "evaluateData = evaluateData.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = trainBusiness.sample(frac=0.7,replace=True)\n",
    "trainBusiness = pd.concat([trainBusiness,q],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our train set, we should train our model with this set. \n",
    "for this purpose, we use Bayes rule.\n",
    "\n",
    "$$ P(c|x) = \\frac{P(x|c)\\times P(c)}{P(x)} $$\n",
    "\n",
    "As we can see, Bayes rule has 4 parts: Posterior, Likelihood, Prior and Evidence.in order to use this rule, we should define each part of the Bayes rule for our project.\n",
    "\n",
    "The posterior probability is the probability of a category given the words in the news. we use Bayes rule to calculate this probability.\n",
    "\n",
    "$$ posterior = {P(category | x_0, x_1, x_2, ...,  x_n)} $$\n",
    "\n",
    "which is $x_n$ is the n-th word of that news.\n",
    "\n",
    "We define the probability of each category as the prior probability which means how probable is it for a news to be for a certain category.\n",
    "\n",
    "We define the likelihood as the probability of each word of a news given the category which means how probable it is for that category to use that word. in other words, the likelihood probability is:\n",
    "\n",
    "$$ likelihood = {P(x_0, x_1, x_2, ...,  x_n|category)} $$\n",
    "\n",
    "Since the probability of existing a word in a certain category is independent of the probability of existing another word in that category for each news, we can multiply these probabilities to calculate our conditional probability.\n",
    "\n",
    "$$ likelihood = {P(x_0|category)\\times P(x_1|category) \\times P(x_2|category) \\times ... \\times P(x_n|category)} $$\n",
    "\n",
    "The evidence is the probabiliy of all words that we have in a given news.\n",
    "\n",
    "$$ evidence = {P(x_0, x_1, x_2, ...,  x_n)} $$\n",
    "\n",
    "Since we are going to compare the posterior probabilities for each category and in each category the evidence probability is the same as other categories, thus we don't need to calculate the evidence probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part $\\mathrm{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to train our model for only two categories. TRAVEL category and BUSINESS category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelWords = trainTravel.words.values\n",
    "travelWords = list(itertools.chain.from_iterable(travelWords))\n",
    "BusinessWords = trainBusiness.words.values\n",
    "BusinessWords = list(itertools.chain.from_iterable(BusinessWords))\n",
    "allTravelAndBusinessWord = list(map(''.join, set(itertools.chain(travelWords, BusinessWords))))\n",
    "travelWordsCount = dict(collections.Counter(travelWords))\n",
    "BusinessWordsCount = dict(collections.Counter(BusinessWords))\n",
    "newDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences'])\n",
    "for word in allTravelAndBusinessWord:\n",
    "    to = 0\n",
    "    bo = 0\n",
    "    if word in travelWordsCount:\n",
    "        to = travelWordsCount[word]\n",
    "    if word in BusinessWordsCount:\n",
    "        bo = BusinessWordsCount[word]\n",
    "    newDataFrame = newDataFrame.append({'Word' : word,'Travel occurrences' : to,'Business occurrences' : bo},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new data frame. for each word, we calculate the probability of that word given the category. in fact, we calculate $ P(word | category) $ for each word in our training set (in this case out training set contain only two mentioned categories).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the conditional probability we use the Laplace Smoothing. since we multiply the conditional probabilities in order to calculate the likelihood, if we have a word that used only one time in one category, the likelihood probability for other categories will be zero even if other conditional probabilities have a high value. Laplace smoothing is used to solve the problem of zero probability.\n",
    "\n",
    "$$ P(word|category) =  \\frac{O(word,category) + \\alpha}{S(words,categor) + |A|\\alpha} $$\n",
    "\n",
    "We use this formula to calculate the conditional probability. O(word, category) is the word occurrences in that category, S(words, category) is the number of all words in that category and |A| is the number of distinct words in our train set.\n",
    "alpha is a constant which is used to solve zero probability problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel occurrences</th>\n",
       "      <th>Business occurrences</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>família</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>join</td>\n",
       "      <td>27</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000244419</td>\n",
       "      <td>0.00037649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bellavista</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3332e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lighter</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>luminous</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>leary</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.11079e-05</td>\n",
       "      <td>1.39441e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fairness</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.44399e-06</td>\n",
       "      <td>4.18323e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>whoever</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.11079e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>irkutsk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3332e-05</td>\n",
       "      <td>4.64803e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>july</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000164428</td>\n",
       "      <td>0.000255642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18023 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Travel occurrences Business occurrences Travel Probability  \\\n",
       "Word                                                                    \n",
       "família                     2                    0          2.222e-05   \n",
       "join                       27                   40        0.000244419   \n",
       "bellavista                  1                    0         1.3332e-05   \n",
       "lighter                     2                    0          2.222e-05   \n",
       "luminous                    2                    0          2.222e-05   \n",
       "...                       ...                  ...                ...   \n",
       "leary                       3                    1        3.11079e-05   \n",
       "fairness                    0                    4        4.44399e-06   \n",
       "whoever                     3                    0        3.11079e-05   \n",
       "irkutsk                     1                    0         1.3332e-05   \n",
       "july                       18                   27        0.000164428   \n",
       "\n",
       "           Business Probability  \n",
       "Word                             \n",
       "família             4.64803e-06  \n",
       "join                 0.00037649  \n",
       "bellavista          4.64803e-06  \n",
       "lighter             4.64803e-06  \n",
       "luminous            4.64803e-06  \n",
       "...                         ...  \n",
       "leary               1.39441e-05  \n",
       "fairness            4.18323e-05  \n",
       "whoever             4.64803e-06  \n",
       "irkutsk             4.64803e-06  \n",
       "july                0.000255642  \n",
       "\n",
       "[18023 rows x 4 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "newDataFrame['Travel Probability'] = (newDataFrame['Travel occurrences'] + alpha) / (newDataFrame['Travel occurrences'].sum() + (len(set(travelWords + BusinessWords))*alpha))\n",
    "newDataFrame['Business Probability'] = (newDataFrame['Business occurrences'] + alpha) / (newDataFrame['Business occurrences'].sum() + (len(set(travelWords + BusinessWords))*alpha))\n",
    "newDataFrame = newDataFrame.set_index('Word')\n",
    "newDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelAndBusinessEvaluateData = pd.concat([evaluateBusiness, evaluateTravel])\n",
    "travelAndBusinessEvaluateData = travelAndBusinessEvaluateData.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the conditional probability our model is ready to test with evaluate data. for each news in evaluate set, we calculate the prior probability and for all of the words in that news, we multiply the conditional probability with prior probability in order to calculate the posterior probability. After that, we can predict the category for each news. the category with higher posterior probability is our model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15799</td>\n",
       "      <td>15799</td>\n",
       "      <td>Wendy Smith, ContributorPioneer and passionate...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2016-10-15</td>\n",
       "      <td>Corporate's Responsibility Toward Social Susta...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/corporate...</td>\n",
       "      <td>Today we live in a world where technology cont...</td>\n",
       "      <td>[today, live, world, technology, continue, sim...</td>\n",
       "      <td>9.665179e-39</td>\n",
       "      <td>2.681452e-37</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10951</td>\n",
       "      <td>10951</td>\n",
       "      <td>simon confino, ContributorFounder Director We-...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>Selfish Donald Trump versus Selfless Mother Te...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/selfish-d...</td>\n",
       "      <td>Mother Teresa has this week become a saint. A ...</td>\n",
       "      <td>[mother, teresa, week, become, saint, saint, s...</td>\n",
       "      <td>1.441085e-96</td>\n",
       "      <td>9.753046e-91</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21037</td>\n",
       "      <td>21037</td>\n",
       "      <td>Erica Firpo, Contributor\\nTravel Journalist ba...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>5 Museums In Rome To Visit While The Sistine C...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/5-museums...</td>\n",
       "      <td>Even during the frenzy of a conclave Rome has ...</td>\n",
       "      <td>[even, frenzy, conclave, rome, art, clergy]</td>\n",
       "      <td>6.904126e-15</td>\n",
       "      <td>2.119022e-17</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9394</td>\n",
       "      <td>9394</td>\n",
       "      <td>Anne Z. Cooke, Contributor\\nTravel &amp; Feature J...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-07-17</td>\n",
       "      <td>Untamed in Ucluelet</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/untamed-i...</td>\n",
       "      <td>At first glance, Ucluelet looks like any other...</td>\n",
       "      <td>[first, glance, ucluelet, look, like, end, roa...</td>\n",
       "      <td>3.286041e-74</td>\n",
       "      <td>9.935506e-87</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2965</td>\n",
       "      <td>2965</td>\n",
       "      <td>Susan Portnoy, ContributorThe Insatiable Traveler</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2015-03-05</td>\n",
       "      <td>11 Great Pre-Trip Prep Tips to Start Your Trav...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/travel-pr...</td>\n",
       "      <td>Alexander Graham Bell once said, \"Before anyth...</td>\n",
       "      <td>[alexander, graham, bell, say, anything, else,...</td>\n",
       "      <td>7.203075e-75</td>\n",
       "      <td>1.098199e-74</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  category  \\\n",
       "15799  15799  Wendy Smith, ContributorPioneer and passionate...  BUSINESS   \n",
       "10951  10951  simon confino, ContributorFounder Director We-...  BUSINESS   \n",
       "21037  21037  Erica Firpo, Contributor\\nTravel Journalist ba...    TRAVEL   \n",
       "9394    9394  Anne Z. Cooke, Contributor\\nTravel & Feature J...    TRAVEL   \n",
       "2965    2965  Susan Portnoy, ContributorThe Insatiable Traveler    TRAVEL   \n",
       "\n",
       "             date                                           headline  \\\n",
       "15799  2016-10-15  Corporate's Responsibility Toward Social Susta...   \n",
       "10951  2016-09-05  Selfish Donald Trump versus Selfless Mother Te...   \n",
       "21037  2013-03-11  5 Museums In Rome To Visit While The Sistine C...   \n",
       "9394   2013-07-17                                Untamed in Ucluelet   \n",
       "2965   2015-03-05  11 Great Pre-Trip Prep Tips to Start Your Trav...   \n",
       "\n",
       "                                                    link  \\\n",
       "15799  https://www.huffingtonpost.com/entry/corporate...   \n",
       "10951  https://www.huffingtonpost.com/entry/selfish-d...   \n",
       "21037  https://www.huffingtonpost.com/entry/5-museums...   \n",
       "9394   https://www.huffingtonpost.com/entry/untamed-i...   \n",
       "2965   https://www.huffingtonpost.com/entry/travel-pr...   \n",
       "\n",
       "                                       short_description  \\\n",
       "15799  Today we live in a world where technology cont...   \n",
       "10951  Mother Teresa has this week become a saint. A ...   \n",
       "21037  Even during the frenzy of a conclave Rome has ...   \n",
       "9394   At first glance, Ucluelet looks like any other...   \n",
       "2965   Alexander Graham Bell once said, \"Before anyth...   \n",
       "\n",
       "                                                   words  Travel Probability  \\\n",
       "15799  [today, live, world, technology, continue, sim...        9.665179e-39   \n",
       "10951  [mother, teresa, week, become, saint, saint, s...        1.441085e-96   \n",
       "21037        [even, frenzy, conclave, rome, art, clergy]        6.904126e-15   \n",
       "9394   [first, glance, ucluelet, look, like, end, roa...        3.286041e-74   \n",
       "2965   [alexander, graham, bell, say, anything, else,...        7.203075e-75   \n",
       "\n",
       "       Business Probability Prediction  \n",
       "15799          2.681452e-37   BUSINESS  \n",
       "10951          9.753046e-91   BUSINESS  \n",
       "21037          2.119022e-17     TRAVEL  \n",
       "9394           9.935506e-87     TRAVEL  \n",
       "2965           1.098199e-74   BUSINESS  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,row in travelAndBusinessEvaluateData.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTravelAndBusinessWord:\n",
    "            travelPriorProbability *= newDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= newDataFrame.at[word,'Business Probability']\n",
    "\n",
    "    travelAndBusinessEvaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    travelAndBusinessEvaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability:\n",
    "        travelAndBusinessEvaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "    else:\n",
    "        travelAndBusinessEvaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "travelAndBusinessEvaluateData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8657303370786517 0.9524103831891224\n",
      "0.9279700654817586 0.8058489033306255\n",
      "0.8890838890838891\n"
     ]
    }
   ],
   "source": [
    "#Travel\n",
    "correctTravel = travelAndBusinessEvaluateData.loc[(travelAndBusinessEvaluateData['category'] == 'TRAVEL') & (travelAndBusinessEvaluateData['Prediction'] == 'TRAVEL')]\n",
    "correctTravel = len(correctTravel)\n",
    "allOfTravel = (travelAndBusinessEvaluateData['category'] == 'TRAVEL').sum()\n",
    "allTravelPrediction = (travelAndBusinessEvaluateData['Prediction'] == 'TRAVEL').sum()\n",
    "travelRecall = correctTravel/allOfTravel\n",
    "travelPrecision = correctTravel/allTravelPrediction\n",
    "print(travelRecall,travelPrecision)\n",
    "#Business\n",
    "correctBusiness = travelAndBusinessEvaluateData.loc[(travelAndBusinessEvaluateData['category'] == 'BUSINESS') & (travelAndBusinessEvaluateData['Prediction'] == 'BUSINESS')]\n",
    "correctBusiness = len(correctBusiness)\n",
    "allOfBusiness = (travelAndBusinessEvaluateData['category'] == 'BUSINESS').sum()\n",
    "allBusinessPrediction = (travelAndBusinessEvaluateData['Prediction'] == 'BUSINESS').sum()\n",
    "BusinessRecall = correctBusiness/allOfBusiness\n",
    "BusinessPrecision = correctBusiness/allBusinessPrediction\n",
    "print(BusinessRecall,BusinessPrecision)\n",
    "travelAndBusinessEvaluateData['correct'] = (travelAndBusinessEvaluateData['category'] == travelAndBusinessEvaluateData['Prediction'])\n",
    "correctDetected = (travelAndBusinessEvaluateData['correct']).sum()\n",
    "accuracy =  correctDetected / len(travelAndBusinessEvaluateData)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| phase1 | Travel  | Business  |\n",
    "| --- | --- | --- |\n",
    "| Recall | %86 | %95 |\n",
    "| Precision | %93 | %80\n",
    "| Accuracy   | %89 |  %89  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part $\\mathrm{II}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add third category to our model. we add STYLE & BEAUTY category to our train set and repeat all the process again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbWords = trainSB.words.values\n",
    "sbWords = list(itertools.chain.from_iterable(sbWords))\n",
    "allTrainDataWords = list(map(''.join, set(itertools.chain(travelWords, BusinessWords,sbWords))))\n",
    "sbWordsCount = dict(collections.Counter(sbWords))\n",
    "travelWordsCount = dict(collections.Counter(travelWords))\n",
    "BusinessWordsCount = dict(collections.Counter(BusinessWords))\n",
    "allDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences','Style & Beauty occurrences'])\n",
    "for word in allTrainDataWords:\n",
    "    to = 0\n",
    "    sbo = 0\n",
    "    bo = 0\n",
    "    if word in travelWordsCount:\n",
    "        to = travelWordsCount[word]\n",
    "    if word in BusinessWordsCount:\n",
    "        bo = BusinessWordsCount[word]\n",
    "    if word in sbWordsCount:\n",
    "        sbo = sbWordsCount[word]\n",
    "    allDataFrame = allDataFrame.append({'Word' : word,'Travel occurrences' : to,'Business occurrences' : bo,'Style & Beauty occurrences' : sbo},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use Laplace smoothing here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel occurrences</th>\n",
       "      <th>Business occurrences</th>\n",
       "      <th>Style &amp; Beauty occurrences</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>luminous</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.17942e-05</td>\n",
       "      <td>4.55496e-06</td>\n",
       "      <td>5.65527e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>refinancers</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.35884e-06</td>\n",
       "      <td>5.01045e-05</td>\n",
       "      <td>5.14115e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tack</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.05119e-05</td>\n",
       "      <td>4.55496e-06</td>\n",
       "      <td>1.54235e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>puzzle</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.92295e-05</td>\n",
       "      <td>4.55496e-06</td>\n",
       "      <td>1.54235e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lineup</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.17942e-05</td>\n",
       "      <td>4.55496e-06</td>\n",
       "      <td>5.14115e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Travel occurrences Business occurrences  \\\n",
       "Word                                                  \n",
       "luminous                     2                    0   \n",
       "refinancers                  0                    5   \n",
       "tack                         3                    0   \n",
       "puzzle                       4                    0   \n",
       "lineup                       2                    0   \n",
       "\n",
       "            Style & Beauty occurrences Travel Probability  \\\n",
       "Word                                                        \n",
       "luminous                             5        2.17942e-05   \n",
       "refinancers                          0        4.35884e-06   \n",
       "tack                                 1        3.05119e-05   \n",
       "puzzle                               1        3.92295e-05   \n",
       "lineup                               0        2.17942e-05   \n",
       "\n",
       "            Business Probability Style & Beauty Probability  \n",
       "Word                                                         \n",
       "luminous             4.55496e-06                5.65527e-05  \n",
       "refinancers          5.01045e-05                5.14115e-06  \n",
       "tack                 4.55496e-06                1.54235e-05  \n",
       "puzzle               4.55496e-06                1.54235e-05  \n",
       "lineup               4.55496e-06                5.14115e-06  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "allDataFrame['Travel Probability'] = (allDataFrame['Travel occurrences'] + alpha) / (allDataFrame['Travel occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Business Probability'] = (allDataFrame['Business occurrences'] + alpha) / (allDataFrame['Business occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Style & Beauty Probability'] = (allDataFrame['Style & Beauty occurrences'] + alpha) / (allDataFrame['Style & Beauty occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame = allDataFrame.set_index('Word')\n",
    "allDataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model with our evaluation set which has all three category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in evaluateData.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    sbPriorProbability = len(trainSB)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTrainDataWords:\n",
    "            travelPriorProbability *= allDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= allDataFrame.at[word,'Business Probability']\n",
    "            sbPriorProbability *= allDataFrame.at[word,'Style & Beauty Probability']\n",
    "    evaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    evaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "    evaluateData.at[index,'Style & Beauty Probability'] = sbPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability and travelPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "    if businessPriorProbability >= travelPriorProbability and businessPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "    if sbPriorProbability >= travelPriorProbability and sbPriorProbability>= businessPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'STYLE & BEAUTY'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16075</td>\n",
       "      <td>16075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-08-13</td>\n",
       "      <td>Victoria Beckham Awkwardly Reunites With Spice...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/spice-gir...</td>\n",
       "      <td>But the power quintet reunited as the Girls of...</td>\n",
       "      <td>[power, quintet, reunite, girls, spice, london...</td>\n",
       "      <td>1.170543e-43</td>\n",
       "      <td>6.798945e-47</td>\n",
       "      <td>9.233991e-42</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4603</td>\n",
       "      <td>4603</td>\n",
       "      <td>Pierre R. Berastaín, Contributor\\nUndocumented...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-09-08</td>\n",
       "      <td>Meeting Amtrak's Cross-Country Passengers</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/amtrak-am...</td>\n",
       "      <td>The different social spaces within the train a...</td>\n",
       "      <td>[different, social, space, within, train, allo...</td>\n",
       "      <td>9.062898e-51</td>\n",
       "      <td>1.988969e-48</td>\n",
       "      <td>1.452220e-55</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12692</td>\n",
       "      <td>12692</td>\n",
       "      <td>Richard Wiese, Contributor\\nHost of Born to Ex...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>Born to Explore: Eating Rotten Shark With A Vi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/rotten-ic...</td>\n",
       "      <td>I'm no food critic, but imagine eating the con...</td>\n",
       "      <td>[food, critic, imagine, eat, content, bait, bo...</td>\n",
       "      <td>1.217144e-42</td>\n",
       "      <td>8.450850e-46</td>\n",
       "      <td>1.020622e-45</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>Mike Arkus, ContributorJournalist</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2016-05-29</td>\n",
       "      <td>The Mediaeval Greek Fortress Town of Monemvasi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-media...</td>\n",
       "      <td>Enough of 'equivalents' already. Lonely Planet...</td>\n",
       "      <td>[enough, equivalents, already, lonely, planet,...</td>\n",
       "      <td>8.168402e-106</td>\n",
       "      <td>1.078084e-119</td>\n",
       "      <td>1.815601e-117</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5394</td>\n",
       "      <td>5394</td>\n",
       "      <td>BnBFinder.com, Contributor\\nBnBFinder</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-09-25</td>\n",
       "      <td>Leaf Peeping From The Porch (PHOTOS)</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/taking-th...</td>\n",
       "      <td>You know it's coming, but every year it still ...</td>\n",
       "      <td>[know, come, every, year, still, feel, like, c...</td>\n",
       "      <td>9.169448e-53</td>\n",
       "      <td>1.312540e-54</td>\n",
       "      <td>1.140317e-53</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  \\\n",
       "16075  16075                                                NaN   \n",
       "4603    4603  Pierre R. Berastaín, Contributor\\nUndocumented...   \n",
       "12692  12692  Richard Wiese, Contributor\\nHost of Born to Ex...   \n",
       "2580    2580                  Mike Arkus, ContributorJournalist   \n",
       "5394    5394              BnBFinder.com, Contributor\\nBnBFinder   \n",
       "\n",
       "             category        date  \\\n",
       "16075  STYLE & BEAUTY  2012-08-13   \n",
       "4603           TRAVEL  2012-09-08   \n",
       "12692          TRAVEL  2012-07-31   \n",
       "2580           TRAVEL  2016-05-29   \n",
       "5394           TRAVEL  2012-09-25   \n",
       "\n",
       "                                                headline  \\\n",
       "16075  Victoria Beckham Awkwardly Reunites With Spice...   \n",
       "4603           Meeting Amtrak's Cross-Country Passengers   \n",
       "12692  Born to Explore: Eating Rotten Shark With A Vi...   \n",
       "2580   The Mediaeval Greek Fortress Town of Monemvasi...   \n",
       "5394                Leaf Peeping From The Porch (PHOTOS)   \n",
       "\n",
       "                                                    link  \\\n",
       "16075  https://www.huffingtonpost.com/entry/spice-gir...   \n",
       "4603   https://www.huffingtonpost.com/entry/amtrak-am...   \n",
       "12692  https://www.huffingtonpost.com/entry/rotten-ic...   \n",
       "2580   https://www.huffingtonpost.com/entry/the-media...   \n",
       "5394   https://www.huffingtonpost.com/entry/taking-th...   \n",
       "\n",
       "                                       short_description  \\\n",
       "16075  But the power quintet reunited as the Girls of...   \n",
       "4603   The different social spaces within the train a...   \n",
       "12692  I'm no food critic, but imagine eating the con...   \n",
       "2580   Enough of 'equivalents' already. Lonely Planet...   \n",
       "5394   You know it's coming, but every year it still ...   \n",
       "\n",
       "                                                   words  Travel Probability  \\\n",
       "16075  [power, quintet, reunite, girls, spice, london...        1.170543e-43   \n",
       "4603   [different, social, space, within, train, allo...        9.062898e-51   \n",
       "12692  [food, critic, imagine, eat, content, bait, bo...        1.217144e-42   \n",
       "2580   [enough, equivalents, already, lonely, planet,...       8.168402e-106   \n",
       "5394   [know, come, every, year, still, feel, like, c...        9.169448e-53   \n",
       "\n",
       "       Business Probability  Style & Beauty Probability      Prediction  \n",
       "16075          6.798945e-47                9.233991e-42  STYLE & BEAUTY  \n",
       "4603           1.988969e-48                1.452220e-55        BUSINESS  \n",
       "12692          8.450850e-46                1.020622e-45          TRAVEL  \n",
       "2580          1.078084e-119               1.815601e-117          TRAVEL  \n",
       "5394           1.312540e-54                1.140317e-53          TRAVEL  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8264044943820225 0.8652941176470588\n",
      "0.9055191768007483 0.7457627118644068\n",
      "0.842832469775475 0.9219143576826196\n",
      "0.8510684692542521\n"
     ]
    }
   ],
   "source": [
    "#Travel\n",
    "correctTravel = evaluateData.loc[(evaluateData['category'] == 'TRAVEL') & (evaluateData['Prediction'] == 'TRAVEL')]\n",
    "correctTravelSum = len(correctTravel)\n",
    "allOfTravel = (evaluateData['category'] == 'TRAVEL').sum()\n",
    "allTravelPrediction = (evaluateData['Prediction'] == 'TRAVEL').sum()\n",
    "travelRecall = correctTravelSum/allOfTravel\n",
    "travelPrecision = correctTravelSum/allTravelPrediction\n",
    "print(travelRecall,travelPrecision)\n",
    "#Business\n",
    "correctBusiness = evaluateData.loc[(evaluateData['category'] == 'BUSINESS') & (evaluateData['Prediction'] == 'BUSINESS')]\n",
    "correctBusinessSum = len(correctBusiness)\n",
    "allOfBusiness = (evaluateData['category'] == 'BUSINESS').sum()\n",
    "allBusinessPrediction = (evaluateData['Prediction'] == 'BUSINESS').sum()\n",
    "BusinessRecall = correctBusinessSum/allOfBusiness\n",
    "BusinessPrecision = correctBusinessSum/allBusinessPrediction\n",
    "print(BusinessRecall,BusinessPrecision)\n",
    "#Style\n",
    "correctSB = evaluateData.loc[(evaluateData['category'] == 'STYLE & BEAUTY') & (evaluateData['Prediction'] == 'STYLE & BEAUTY')]\n",
    "correctSBSum = len(correctSB)\n",
    "allOfSB = (evaluateData['category'] == 'STYLE & BEAUTY').sum()\n",
    "allSBPrediction = (evaluateData['Prediction'] == 'STYLE & BEAUTY').sum()\n",
    "SBRecall = correctSBSum/allOfSB\n",
    "SBPrecision = correctSBSum/allSBPrediction\n",
    "print(SBRecall,SBPrecision)\n",
    "\n",
    "evaluateData['correct'] = (evaluateData['category'] == evaluateData['Prediction'])\n",
    "correctDetected = (evaluateData['correct']).sum()\n",
    "accuracy =  correctDetected / len(evaluateData)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| phase1 | Travel  | Business | Style & Beauty |\n",
    "| --- | --- | --- | --- |\n",
    "| Recall | %83 | %90 | %84 |\n",
    "| Precision | %86 | %74 | %92 |\n",
    "| Accuracy   | %85 | %85 | %85 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is a summary of prediction results on a classification problem.each row of the matrix corresponds to a predicted class and each column of the matrix corresponds to an actual class. confusion matrix helps us to have a better view of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAVEL:  1471 221 88\n",
      "BUSINESS:  968 65 36\n",
      "STYLE & BEAUTY:  1464 164 109\n"
     ]
    }
   ],
   "source": [
    "wrongTravelB = evaluateData.loc[(evaluateData['category'] == 'TRAVEL') & (evaluateData['Prediction'] == 'BUSINESS')]\n",
    "wrongTravelS = evaluateData.loc[(evaluateData['category'] == 'TRAVEL') & (evaluateData['Prediction'] == 'STYLE & BEAUTY')]\n",
    "wrongBusinessT = evaluateData.loc[(evaluateData['category'] == 'BUSINESS') & (evaluateData['Prediction'] == 'TRAVEL')]\n",
    "wrongBusinessS = evaluateData.loc[(evaluateData['category'] == 'BUSINESS') & (evaluateData['Prediction'] == 'STYLE & BEAUTY')]\n",
    "wrongSBT = evaluateData.loc[(evaluateData['category'] == 'STYLE & BEAUTY') & (evaluateData['Prediction'] == 'TRAVEL')]\n",
    "wrongSBB = evaluateData.loc[(evaluateData['category'] == 'STYLE & BEAUTY') & (evaluateData['Prediction'] == 'BUSINESS')]\n",
    "print('TRAVEL: ',len(correctTravel),len(wrongTravelB),len(wrongTravelS))\n",
    "print('BUSINESS: ',len(correctBusiness),len(wrongBusinessT),len(wrongBusinessS))\n",
    "print('STYLE & BEAUTY: ',len(correctSB),len(wrongSBT),len(wrongSBB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Travel  | Business | Style & Beauty |\n",
    "| --- | --- | --- | --- |\n",
    "| Travel | 1471 | 221 | 88 |\n",
    "| Business | 65 | 968 | 36 |\n",
    "| Style & Beauty   | 164 | 109 | 1464 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the recall and precision for each category we see that there is a difference between these values. this is because, in our training data, the distribution between all categories is not the same and some class exists more than the others. for overcoming this problem, we have two options. we can randomly duplicate examples in the minority class or randomly delete examples in the majority class. after that, we have a balance data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset (test.csv) which is similar to our train dataset but has no category. we want to predict the category of news for this dataset with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataFrame = pd.read_csv('Attachment/test.csv')\n",
    "testDataFrame = cleanText(testDataFrame)\n",
    "testDataFrame = testDataFrame.dropna(subset=['short_description'])\n",
    "for index,row in testDataFrame.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    sbPriorProbability = len(trainSB)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTrainDataWords:\n",
    "            travelPriorProbability *= allDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= allDataFrame.at[word,'Business Probability']\n",
    "            sbPriorProbability *= allDataFrame.at[word,'Style & Beauty Probability']\n",
    "    testDataFrame.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    testDataFrame.at[index,'Business Probability'] = businessPriorProbability\n",
    "    testDataFrame.at[index,'Style & Beauty Probability'] = sbPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability and travelPriorProbability >= sbPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'TRAVEL'\n",
    "    if businessPriorProbability >= travelPriorProbability and businessPriorProbability >= sbPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'BUSINESS'\n",
    "    if sbPriorProbability >= travelPriorProbability and sbPriorProbability>= businessPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'STYLE & BEAUTY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the category prediction into the ouptut.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2543</td>\n",
       "      <td>2543</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2544</td>\n",
       "      <td>2544</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2545</td>\n",
       "      <td>2545</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2546</td>\n",
       "      <td>2546</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2547</td>\n",
       "      <td>2547</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        category\n",
       "0         0  STYLE & BEAUTY\n",
       "1         1          TRAVEL\n",
       "4         4  STYLE & BEAUTY\n",
       "5         5          TRAVEL\n",
       "6         6  STYLE & BEAUTY\n",
       "...     ...             ...\n",
       "2543   2543        BUSINESS\n",
       "2544   2544          TRAVEL\n",
       "2545   2545        BUSINESS\n",
       "2546   2546          TRAVEL\n",
       "2547   2547        BUSINESS\n",
       "\n",
       "[2419 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({\"index\": testDataFrame['index'],\"category\": testDataFrame['category']})\n",
    "output.to_csv('output.csv', index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. On the other hand, Lemmatization reduces the inflected words properly ensuring that the root word belongs to the language. The result of Lemmatization is called lemma which is dictionary form, or citation form of the words. \n",
    "We use Lemmatization in our project because we interested in word frequency and want to treat all forms of a word as one. in stemming some forms of a word may not have the same stem and that can influence the frequency of that word and the accuracy of our model as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf is short for \"term frequency-inverse document frequency\" which basically reflects how important a word is to a document.\n",
    "tf measures how frequently a term occurs in a document.\n",
    "\n",
    "TF = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "idf measures how important a term is.\n",
    "\n",
    "IDF = $log_e$(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "In Bayesian model, when we calculate the probabilities based on each word occurrences, each word in each document (in our case each news) counted as one. if we want to use tf-idf, instead of counting each word as one, we use the tf-idf weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a hundred percent precision for certain a category even if our model cannot predict the categories correctly. for example, if we predict one TRAVEL news correctly and then assign all other news to other categories (e.g BUSINESS) then the precision of the TRAVEL will be a hundred percent although our predictions are wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We referred to this problem earlier and the Laplace smoothing as a solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Mehrdad': virtualenv)",
   "language": "python",
   "name": "python37464bitmehrdadvirtualenv5fa8cbb105824cae8f34d6b894d7a675"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

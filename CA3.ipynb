{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - Computer Assignment 3 - Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehrdad Nourbakhsh(810194418)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we want to use Bayes rule for classifying news based on a short description of that news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with nearly 23000 news. each news has a category. we want to create a model to classify the category of that news based on the short description of that news. we read the dataset from .csv file and load that into a panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Påskekrim is merely the tip of the proverbial ...  \n",
       "1                                                NaN  \n",
       "2  Madonna is slinking her way into footwear now,...  \n",
       "3  But what if you're a 30-something couple that ...  \n",
       "4  Obamacare was supposed to make birth control f...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_csv('Attachment/data.csv')\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each news has some other information too rather than the short description (e.g headline, authors). but we ignore them and use only words of the short description to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should preprocess the data and use the result for the model. we want to normalize the short description.\n",
    "we use nltk library for normalizing the text.normalization include these steps:\n",
    "* converting uppercase to lowercase\n",
    "* removing punctuation signs\n",
    "* removing numbers\n",
    "* converting each text into a list of words\n",
    "* removing stop words\n",
    "* using lemmatization to remove inflectional endings only and to return the base form of a word\n",
    "\n",
    "If we don't convert all words to lowercase, our model might treat a word which is at the beginning of a sentence with a capital letter, different from the same word which appears later in the sentence but without any capital latter. this might influence our model accuracy. thus we convert all letters to lowercase.\n",
    "\n",
    "In our model, we use words frequency and occurrences of them in the text. we want to find relevant results not only for the exact expression but also for the other possible forms of the words we used. for this purpose we use lemmatization. lemmatization helps us to treat all possible forms of a word as an individual word and this can improve our model with increasing word frequency for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>[påskekrim, merely, tip, proverbial, iceberg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>[madonna, slink, way, footwear, truth, dare, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>[something, couple, shy, away, table, dance, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>[obamacare, suppose, make, birth, control, fre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Påskekrim is merely the tip of the proverbial ...   \n",
       "1                                                NaN   \n",
       "2  Madonna is slinking her way into footwear now,...   \n",
       "3  But what if you're a 30-something couple that ...   \n",
       "4  Obamacare was supposed to make birth control f...   \n",
       "\n",
       "                                               words  \n",
       "0  [påskekrim, merely, tip, proverbial, iceberg, ...  \n",
       "1                                                 []  \n",
       "2  [madonna, slink, way, footwear, truth, dare, p...  \n",
       "3  [something, couple, shy, away, table, dance, e...  \n",
       "4  [obamacare, suppose, make, birth, control, fre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataFrame.short_description\n",
    "data = data.fillna('')\n",
    "data = data.str.lower()\n",
    "data = data.str.replace('[^\\w\\s]',' ')\n",
    "data = data.str.replace('\\d+', '')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordsList = data.apply(word_tokenize)\n",
    "wordsList = wordsList.apply(lambda x: [item for item in x if item not in stopWords])\n",
    "wordsList = wordsList.apply(lambda x: [lemmatizer.lemmatize(y, pos=\"v\") for y in x])\n",
    "dataFrame['words'] = wordsList\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to separate our data into two parts. the train set and evaluate set. we use 80 percent of data as train set and 20 percent as evaluate set. we want to have all sorts of news. if we choose 80 percent of real data as train set, we may have a set with only one or two categories, therefore, our model can not detect other categories as well. we have to create a train set with good diversity from all of the categories. \n",
    "\n",
    "for each category, we choose a random subset of that category (80 percent) and then combine those subsets to create our train set and the remaining 20 percent of each category for evaluate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2827</td>\n",
       "      <td>2827</td>\n",
       "      <td>Michelle Persad</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-10-23</td>\n",
       "      <td>Jessica Alba Pulls Off The Perfect Combination...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jessica-a...</td>\n",
       "      <td>Want more? Be sure to check out Stylelist on T...</td>\n",
       "      <td>[want, sure, check, stylelist, twitter, facebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16662</td>\n",
       "      <td>16662</td>\n",
       "      <td>Dr. Steve Rosenberg, Contributor\\nPodiatrist, ...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2013-11-25</td>\n",
       "      <td>Solutions to Prevent Tired Feet and Legs Durin...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/solutions...</td>\n",
       "      <td>It is holiday time again along with the long l...</td>\n",
       "      <td>[holiday, time, along, long, line, wait, buy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4318</td>\n",
       "      <td>4318</td>\n",
       "      <td>Mark Gongloff</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-08-01</td>\n",
       "      <td>Why A Higher Unemployment Rate Is Actually Goo...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/unemploym...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20213</td>\n",
       "      <td>20213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>Virgin Galactic Gets FAA Approval To Test Spac...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/virgin-ga...</td>\n",
       "      <td>By: Brian Berger, Space News Published: 05/31/...</td>\n",
       "      <td>[brian, berger, space, news, publish, edt, spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20577</td>\n",
       "      <td>20577</td>\n",
       "      <td>24/7 Wall St., 24/7 Wall St.</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-01-25</td>\n",
       "      <td>States With The Least Government Benefits: 24/...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/states-go...</td>\n",
       "      <td>Right now, the states already bear a substanti...</td>\n",
       "      <td>[right, state, already, bear, substantial, bur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  \\\n",
       "2827    2827                                    Michelle Persad   \n",
       "16662  16662  Dr. Steve Rosenberg, Contributor\\nPodiatrist, ...   \n",
       "4318    4318                                      Mark Gongloff   \n",
       "20213  20213                                                NaN   \n",
       "20577  20577                       24/7 Wall St., 24/7 Wall St.   \n",
       "\n",
       "             category        date  \\\n",
       "2827   STYLE & BEAUTY  2012-10-23   \n",
       "16662  STYLE & BEAUTY  2013-11-25   \n",
       "4318         BUSINESS  2014-08-01   \n",
       "20213          TRAVEL  2012-05-31   \n",
       "20577        BUSINESS  2014-01-25   \n",
       "\n",
       "                                                headline  \\\n",
       "2827   Jessica Alba Pulls Off The Perfect Combination...   \n",
       "16662  Solutions to Prevent Tired Feet and Legs Durin...   \n",
       "4318   Why A Higher Unemployment Rate Is Actually Goo...   \n",
       "20213  Virgin Galactic Gets FAA Approval To Test Spac...   \n",
       "20577  States With The Least Government Benefits: 24/...   \n",
       "\n",
       "                                                    link  \\\n",
       "2827   https://www.huffingtonpost.com/entry/jessica-a...   \n",
       "16662  https://www.huffingtonpost.com/entry/solutions...   \n",
       "4318   https://www.huffingtonpost.com/entry/unemploym...   \n",
       "20213  https://www.huffingtonpost.com/entry/virgin-ga...   \n",
       "20577  https://www.huffingtonpost.com/entry/states-go...   \n",
       "\n",
       "                                       short_description  \\\n",
       "2827   Want more? Be sure to check out Stylelist on T...   \n",
       "16662  It is holiday time again along with the long l...   \n",
       "4318                                                 NaN   \n",
       "20213  By: Brian Berger, Space News Published: 05/31/...   \n",
       "20577  Right now, the states already bear a substanti...   \n",
       "\n",
       "                                                   words  \n",
       "2827   [want, sure, check, stylelist, twitter, facebo...  \n",
       "16662  [holiday, time, along, long, line, wait, buy, ...  \n",
       "4318                                                  []  \n",
       "20213  [brian, berger, space, news, publish, edt, spa...  \n",
       "20577  [right, state, already, bear, substantial, bur...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "travel = dataFrame[dataFrame['category'] == 'TRAVEL']\n",
    "business = dataFrame[dataFrame['category'] == 'BUSINESS']\n",
    "sb = dataFrame[dataFrame['category'] == 'STYLE & BEAUTY']\n",
    "trainTravel = travel.sample(frac=0.8)\n",
    "evaluateTravel = travel.drop(trainTravel.index)\n",
    "trainBusiness = business.sample(frac=0.8)\n",
    "evaluateBusiness = business.drop(trainBusiness.index)\n",
    "trainSB = sb.sample(frac=0.8)\n",
    "evaluateSB = sb.drop(trainSB.index)\n",
    "trainData = pd.concat([trainTravel,trainBusiness, trainSB])\n",
    "trainData = trainData.sample(frac=1)\n",
    "evaluateData = pd.concat([evaluateSB,evaluateBusiness, evaluateTravel])\n",
    "evaluateData = evaluateData.sample(frac=1)\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our train set, we should train our model with this set. \n",
    "for this purpose, we use Bayes rule.\n",
    "\n",
    "$$ P(c|x) = \\frac{P(x|c)\\times P(c)}{P(x)} $$\n",
    "\n",
    "As we can see, Bayes rule has 4 parts: Posterior, Likelihood, Prior and Evidence.in order to use this rule, we should define each part of the Bayes rule for our project.\n",
    "\n",
    "The posterior probability is the probability of a category given the words in the news. we use Bayes rule to calculate this probability.\n",
    "$$ posterior = {P(category | x_0, x_1, x_2, ...,  x_n)} $$\n",
    "which is $x_n$ is the n-th word of that news.\n",
    "\n",
    "We define the probability of each category as the prior probability which means how probable is it for a news to be for a certain category.\n",
    "\n",
    "We define the likelihood as the probability of each word of a news given the category which means how probable it is for that category to use that word. in other words, the likelihood probability is:\n",
    "$$ likelihood = {P(x_0, x_1, x_2, ...,  x_n|category)} $$\n",
    "Since the probability of existing a word in a certain category is independent of the probability of existing another word in that category for each news, we can multiply these probabilities to calculate our conditional probability.\n",
    "$$ likelihood = {P(x_0|category)\\times P(x_1|category) \\times P(x_2|category) \\times ... \\times P(x_n|category)} $$\n",
    "\n",
    "The evidence is the probabiliy of all words that we have in a given news.\n",
    "$$ evidence = {P(x_0, x_1, x_2, ...,  x_n)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part $\\mathrm{I}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103142 51994 17700\n"
     ]
    }
   ],
   "source": [
    "travelWords = trainTravel.words.values\n",
    "travelWords = list(itertools.chain.from_iterable(travelWords))\n",
    "BusinessWords = trainBusiness.words.values\n",
    "BusinessWords = list(itertools.chain.from_iterable(BusinessWords))\n",
    "allTravelAndBusinessWord = list(map(''.join, set(itertools.chain(travelWords, BusinessWords))))\n",
    "print(len(travelWords),len(BusinessWords),len(allTravelAndBusinessWord))\n",
    "# newDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences'])\n",
    "# for word in allTravelAndBusinessWord:\n",
    "#     newDataFrame = newDataFrame.append({'Word' : word,'Travel occurrences' : travelWords.count(word),'Business occurrences' : BusinessWords.count(word)},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newDataFrame['Travel Probability'] = newDataFrame['Travel occurrences'] / newDataFrame['Travel occurrences'].sum()\n",
    "# newDataFrame['Business Probability'] = newDataFrame['Business occurrences'] / newDataFrame['Business occurrences'].sum()\n",
    "# newDataFrame = newDataFrame.set_index('Word')\n",
    "# newDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travelAndBusinessEvaluateData = pd.concat([evaluateBusiness, evaluateTravel])\n",
    "# travelAndBusinessEvaluateData = travelAndBusinessEvaluateData.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index,row in travelAndBusinessEvaluateData.iterrows():\n",
    "#     travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness))\n",
    "#     businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness))\n",
    "#     words = set(row['words'])\n",
    "#     for word in words:\n",
    "#         if word in allTravelAndBusinessWord:\n",
    "#             travelPriorProbability *= newDataFrame.at[word,'Travel Probability']\n",
    "#             businessPriorProbability *= newDataFrame.at[word,'Business Probability']\n",
    "\n",
    "#     travelAndBusinessEvaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "#     travelAndBusinessEvaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "#     if travelPriorProbability >= businessPriorProbability:\n",
    "#         travelAndBusinessEvaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "#     else:\n",
    "#         travelAndBusinessEvaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "# travelAndBusinessEvaluateData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travelAndBusinessEvaluateData['correct'] = (travelAndBusinessEvaluateData['category'] == travelAndBusinessEvaluateData['Prediction'])\n",
    "# correctDetected = (travelAndBusinessEvaluateData['correct']).sum()\n",
    "# accuracy =  correctDetected / len(travelAndBusinessEvaluateData)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103142 51994 86049 22233\n",
      "14342 8703 11374 22233\n"
     ]
    }
   ],
   "source": [
    "sbWords = trainSB.words.values\n",
    "sbWords = list(itertools.chain.from_iterable(sbWords))\n",
    "allTrainDataWords = list(map(''.join, set(itertools.chain(travelWords, BusinessWords,sbWords))))\n",
    "print(len(travelWords),len(BusinessWords),len(sbWords),len(allTrainDataWords))\n",
    "sbWordsCount = dict(collections.Counter(sbWords))\n",
    "travelWordsCount = dict(collections.Counter(travelWords))\n",
    "BusinessWordsCount = dict(collections.Counter(BusinessWords))\n",
    "print(len(travelWordsCount),len(BusinessWordsCount),len(sbWordsCount),len(allTrainDataWords))\n",
    "allDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences','Style & Beauty occurrences'])\n",
    "for word in allTrainDataWords:\n",
    "    to = 0\n",
    "    sbo = 0\n",
    "    bo = 0\n",
    "    if word in travelWordsCount:\n",
    "        to = travelWordsCount[word]\n",
    "    if word in BusinessWordsCount:\n",
    "        bo = BusinessWordsCount[word]\n",
    "    if word in sbWordsCount:\n",
    "        sbo = sbWordsCount[word]\n",
    "    allDataFrame = allDataFrame.append({'Word' : word,'Travel occurrences' : to,'Business occurrences' : bo,'Style & Beauty occurrences' : sbo},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel occurrences</th>\n",
       "      <th>Business occurrences</th>\n",
       "      <th>Style &amp; Beauty occurrences</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>midways</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.31281e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>injuries</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37604e-06</td>\n",
       "      <td>1.58452e-05</td>\n",
       "      <td>1.02917e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>comme</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37604e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1.02917e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.31281e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>binders</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37604e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1.02917e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iphones</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.31281e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>2.05834e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>whas</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.37604e-06</td>\n",
       "      <td>1.58452e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dprk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.31281e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>see</td>\n",
       "      <td>333</td>\n",
       "      <td>105</td>\n",
       "      <td>491</td>\n",
       "      <td>0.00291882</td>\n",
       "      <td>0.00166375</td>\n",
       "      <td>0.00505323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nilou</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.31281e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22233 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Travel occurrences Business occurrences Style & Beauty occurrences  \\\n",
       "Word                                                                          \n",
       "midways                   1                    0                          0   \n",
       "injuries                  0                    1                          1   \n",
       "comme                     0                    0                          1   \n",
       "ct                        1                    0                          0   \n",
       "binders                   0                    0                          1   \n",
       "...                     ...                  ...                        ...   \n",
       "iphones                   1                    0                          2   \n",
       "whas                      0                    1                          0   \n",
       "dprk                      1                    0                          0   \n",
       "see                     333                  105                        491   \n",
       "nilou                     1                    0                          0   \n",
       "\n",
       "         Travel Probability Business Probability Style & Beauty Probability  \n",
       "Word                                                                         \n",
       "midways         1.31281e-05                    0                          0  \n",
       "injuries        4.37604e-06          1.58452e-05                1.02917e-05  \n",
       "comme           4.37604e-06                    0                1.02917e-05  \n",
       "ct              1.31281e-05                    0                          0  \n",
       "binders         4.37604e-06                    0                1.02917e-05  \n",
       "...                     ...                  ...                        ...  \n",
       "iphones         1.31281e-05                    0                2.05834e-05  \n",
       "whas            4.37604e-06          1.58452e-05                          0  \n",
       "dprk            1.31281e-05                    0                          0  \n",
       "see              0.00291882           0.00166375                 0.00505323  \n",
       "nilou           1.31281e-05                    0                          0  \n",
       "\n",
       "[22233 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "allDataFrame['Travel Probability'] = (allDataFrame['Travel occurrences'] + alpha) / (allDataFrame['Travel occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Business Probability'] = allDataFrame['Business occurrences'] / (allDataFrame['Business occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Style & Beauty Probability'] = allDataFrame['Style & Beauty occurrences'] / (allDataFrame['Style & Beauty occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame = allDataFrame.set_index('Word')\n",
    "allDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in evaluateData.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    sbPriorProbability = len(trainSB)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTrainDataWords:\n",
    "            travelPriorProbability *= allDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= allDataFrame.at[word,'Business Probability']\n",
    "            sbPriorProbability *= allDataFrame.at[word,'Style & Beauty Probability']\n",
    "    evaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    evaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "    evaluateData.at[index,'Style & Beauty Probability'] = sbPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability and travelPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "    if businessPriorProbability >= travelPriorProbability and businessPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "    if sbPriorProbability >= travelPriorProbability and sbPriorProbability>= businessPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'STYLE & BEAUTY'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "      <td>Mary Kincaid, Contributor\\nFounder and Editor ...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>Weekly Roundup of eBay Vintage Clothing Finds</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/weekly-ro...</td>\n",
       "      <td>This week's selections include pieces by YSL, ...</td>\n",
       "      <td>[week, selections, include, piece, ysl, moschi...</td>\n",
       "      <td>6.168558e-53</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.843276e-45</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17924</td>\n",
       "      <td>17924</td>\n",
       "      <td>Oyster, ContributorVisiting, photographing, re...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>The Sexiest Hotels in the Dominican Republic</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-sexie...</td>\n",
       "      <td>Although language can be a barrier for non-Spa...</td>\n",
       "      <td>[although, language, barrier, non, spanish, sp...</td>\n",
       "      <td>3.239074e-49</td>\n",
       "      <td>4.812035e-53</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8893</td>\n",
       "      <td>8893</td>\n",
       "      <td>Susan Fogwell, Contributor\\nLifestyle &amp; Travel...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-02-06</td>\n",
       "      <td>How To Plan A Caribbean Sailing Trip</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/planning-...</td>\n",
       "      <td>For experienced sailors, it is thrilling to sa...</td>\n",
       "      <td>[experience, sailors, thrill, sail, among, fou...</td>\n",
       "      <td>4.976807e-98</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>1657</td>\n",
       "      <td>Suzy Strutner</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>Turns Out Airplane 'Oxygen Masks' Aren't Exact...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/turns-out...</td>\n",
       "      <td>Frequent travelers know that in the event of c...</td>\n",
       "      <td>[frequent, travelers, know, event, change, cab...</td>\n",
       "      <td>1.759336e-45</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11392</td>\n",
       "      <td>11392</td>\n",
       "      <td>24/7 Wall St., 24/7 Wall St.</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2013-04-13</td>\n",
       "      <td>America's Fattest Cities: 24/7 Wall St.</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>Click here to see America’s fattest cities 24/...</td>\n",
       "      <td>[click, see, america, fattest, cities, wall, s...</td>\n",
       "      <td>4.876743e-41</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  \\\n",
       "503      503  Mary Kincaid, Contributor\\nFounder and Editor ...   \n",
       "17924  17924  Oyster, ContributorVisiting, photographing, re...   \n",
       "8893    8893  Susan Fogwell, Contributor\\nLifestyle & Travel...   \n",
       "1657    1657                                      Suzy Strutner   \n",
       "11392  11392                       24/7 Wall St., 24/7 Wall St.   \n",
       "\n",
       "             category        date  \\\n",
       "503    STYLE & BEAUTY  2013-06-25   \n",
       "17924          TRAVEL  2015-01-16   \n",
       "8893           TRAVEL  2012-02-06   \n",
       "1657           TRAVEL  2015-04-23   \n",
       "11392        BUSINESS  2013-04-13   \n",
       "\n",
       "                                                headline  \\\n",
       "503        Weekly Roundup of eBay Vintage Clothing Finds   \n",
       "17924       The Sexiest Hotels in the Dominican Republic   \n",
       "8893                How To Plan A Caribbean Sailing Trip   \n",
       "1657   Turns Out Airplane 'Oxygen Masks' Aren't Exact...   \n",
       "11392            America's Fattest Cities: 24/7 Wall St.   \n",
       "\n",
       "                                                    link  \\\n",
       "503    https://www.huffingtonpost.com/entry/weekly-ro...   \n",
       "17924  https://www.huffingtonpost.com/entry/the-sexie...   \n",
       "8893   https://www.huffingtonpost.com/entry/planning-...   \n",
       "1657   https://www.huffingtonpost.com/entry/turns-out...   \n",
       "11392  https://www.huffingtonpost.com/entry/americas-...   \n",
       "\n",
       "                                       short_description  \\\n",
       "503    This week's selections include pieces by YSL, ...   \n",
       "17924  Although language can be a barrier for non-Spa...   \n",
       "8893   For experienced sailors, it is thrilling to sa...   \n",
       "1657   Frequent travelers know that in the event of c...   \n",
       "11392  Click here to see America’s fattest cities 24/...   \n",
       "\n",
       "                                                   words  Travel Probability  \\\n",
       "503    [week, selections, include, piece, ysl, moschi...        6.168558e-53   \n",
       "17924  [although, language, barrier, non, spanish, sp...        3.239074e-49   \n",
       "8893   [experience, sailors, thrill, sail, among, fou...        4.976807e-98   \n",
       "1657   [frequent, travelers, know, event, change, cab...        1.759336e-45   \n",
       "11392  [click, see, america, fattest, cities, wall, s...        4.876743e-41   \n",
       "\n",
       "       Business Probability  Style & Beauty Probability      Prediction  \n",
       "503            0.000000e+00                4.843276e-45  STYLE & BEAUTY  \n",
       "17924          4.812035e-53                0.000000e+00          TRAVEL  \n",
       "8893           0.000000e+00                0.000000e+00          TRAVEL  \n",
       "1657           0.000000e+00                0.000000e+00          TRAVEL  \n",
       "11392          0.000000e+00                0.000000e+00          TRAVEL  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7431312690798081\n",
      "0.34605442104875317\n",
      "583\n",
      "0.4712277485992446\n"
     ]
    }
   ],
   "source": [
    "evaluateData['Correct'] = (evaluateData['category'] == evaluateData['Prediction'])\n",
    "correctDetected = (evaluateData['Correct']).sum()\n",
    "accuracy =  correctDetected / len(evaluateData)\n",
    "AllOfTravel = (evaluateData['category'] == 'TRAVEL').sum()\n",
    "AllOfBusiness = (evaluateData['category'] == 'BUSINESS').sum()\n",
    "AllOfSB = (evaluateData['category'] == 'STYLE & BEAUTY').sum()\n",
    "correctTravel = (evaluateData.loc[(evaluateData['Correct'] == True) & (dataFrame['category'] == 'TRAVEL')].all(axis='columns')).sum()\n",
    "correctBusiness = (evaluateData.loc[(evaluateData['Correct'] == True) & (dataFrame['category'] == 'BUSINESS')].all(axis='columns')).sum()\n",
    "correctSB = (evaluateData.loc[(evaluateData['Correct'] == True) & (dataFrame['category'] == 'STYLE & BEAUTY')].all(axis='columns')).sum()\n",
    "AllTravelDetected = (evaluateData['Prediction'] == 'TRAVEL').sum()\n",
    "AllSBDetected = (evaluateData['Prediction'] == 'STYLE & BEAUTY').sum()\n",
    "AllBusinessDetected = (evaluateData['Prediction'] == 'BUSINESS').sum()\n",
    "recall1 = (correctTravel) / (AllOfTravel)\n",
    "recall2 = correctBusiness / AllOfBusiness\n",
    "recall3 = correctSB / AllOfSB\n",
    "recall = recall1 + recall2 + recall3\n",
    "precision1 = (correctTravel) / (AllTravelDetected)\n",
    "precision2 = correctBusiness / AllBusinessDetected\n",
    "precision3 = correctSB / AllSBDetected\n",
    "precision = precision1 + precision2 + precision3\n",
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Mehrdad': virtualenv)",
   "language": "python",
   "name": "python37464bitmehrdadvirtualenv5fa8cbb105824cae8f34d6b894d7a675"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

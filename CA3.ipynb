{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - Computer Assignment 3 - Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehrdad Nourbakhsh(810194418)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we want to use Bayes rule for classifying news based on a short description of that news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with nearly 23000 news. each news has a category. we want to create a model to classify the category of that news based on the short description of that news. we read the dataset from .csv file and load that into a panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Påskekrim is merely the tip of the proverbial ...  \n",
       "1                                                NaN  \n",
       "2  Madonna is slinking her way into footwear now,...  \n",
       "3  But what if you're a 30-something couple that ...  \n",
       "4  Obamacare was supposed to make birth control f...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_csv('Attachment/data.csv')\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each news has some other information too rather than the short description (e.g headline, authors). but we ignore them and use only words of the short description to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should preprocess the data and use the result for the model. we want to normalize the short description.\n",
    "we use nltk library for normalizing the text.normalization include these steps:\n",
    "* converting uppercase to lowercase\n",
    "* removing punctuation signs\n",
    "* removing numbers\n",
    "* converting each text into a list of words\n",
    "* removing stop words\n",
    "* using lemmatization to remove inflectional endings only and to return the base form of a word\n",
    "\n",
    "If we don't convert all words to lowercase, our model might treat a word which is at the beginning of a sentence with a capital letter, different from the same word which appears later in the sentence but without any capital latter. this might influence our model accuracy. thus we convert all letters to lowercase.\n",
    "\n",
    "In our model, we use words frequency and occurrences of them in the text. we want to find relevant results not only for the exact expression but also for the other possible forms of the words we used. for this purpose we use lemmatization. lemmatization helps us to treat all possible forms of a word as an individual word and this can improve our model with increasing word frequency for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "      <td>[påskekrim, merely, tip, proverbial, iceberg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "      <td>[madonna, slink, way, footwear, truth, dare, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "      <td>[something, couple, shy, away, table, dance, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "      <td>[obamacare, suppose, make, birth, control, fre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  Påskekrim is merely the tip of the proverbial ...   \n",
       "1                                                NaN   \n",
       "2  Madonna is slinking her way into footwear now,...   \n",
       "3  But what if you're a 30-something couple that ...   \n",
       "4  Obamacare was supposed to make birth control f...   \n",
       "\n",
       "                                               words  \n",
       "0  [påskekrim, merely, tip, proverbial, iceberg, ...  \n",
       "1                                                 []  \n",
       "2  [madonna, slink, way, footwear, truth, dare, p...  \n",
       "3  [something, couple, shy, away, table, dance, e...  \n",
       "4  [obamacare, suppose, make, birth, control, fre...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanText(df):\n",
    "    \n",
    "    data = df.short_description\n",
    "    data = data.fillna('')\n",
    "    data = data.str.lower()\n",
    "    data = data.str.replace('[^\\w\\s]',' ')\n",
    "    data = data.str.replace('\\d+', '')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordsList = data.apply(word_tokenize)\n",
    "    wordsList = wordsList.apply(lambda x: [item for item in x if item not in stopWords])\n",
    "    wordsList = wordsList.apply(lambda x: [lemmatizer.lemmatize(y, pos=\"v\") for y in x])\n",
    "    df['words'] = wordsList\n",
    "    return df\n",
    "\n",
    "dataFrame = cleanText(dataFrame)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to separate our data into two parts. the train set and evaluate set. we use 80 percent of data as train set and 20 percent as evaluate set. we want to have all sorts of news. if we choose 80 percent of real data as train set, we may have a set with only one or two categories, therefore, our model can not detect other categories as well. we have to create a train set with good diversity from all of the categories. \n",
    "\n",
    "for each category, we choose a random subset of that category (80 percent) and then combine those subsets to create our train set and the remaining 20 percent of each category for evaluate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel = dataFrame[dataFrame['category'] == 'TRAVEL']\n",
    "business = dataFrame[dataFrame['category'] == 'BUSINESS']\n",
    "sb = dataFrame[dataFrame['category'] == 'STYLE & BEAUTY']\n",
    "trainTravel = travel.sample(frac=0.8)\n",
    "evaluateTravel = travel.drop(trainTravel.index)\n",
    "trainBusiness = business.sample(frac=0.8)\n",
    "evaluateBusiness = business.drop(trainBusiness.index)\n",
    "trainSB = sb.sample(frac=0.8)\n",
    "evaluateSB = sb.drop(trainSB.index)\n",
    "evaluateData = pd.concat([evaluateSB,evaluateBusiness, evaluateTravel])\n",
    "evaluateData = evaluateData.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our train set, we should train our model with this set. \n",
    "for this purpose, we use Bayes rule.\n",
    "\n",
    "$$ P(c|x) = \\frac{P(x|c)\\times P(c)}{P(x)} $$\n",
    "\n",
    "As we can see, Bayes rule has 4 parts: Posterior, Likelihood, Prior and Evidence.in order to use this rule, we should define each part of the Bayes rule for our project.\n",
    "\n",
    "The posterior probability is the probability of a category given the words in the news. we use Bayes rule to calculate this probability.\n",
    "\n",
    "$$ posterior = {P(category | x_0, x_1, x_2, ...,  x_n)} $$\n",
    "\n",
    "which is $x_n$ is the n-th word of that news.\n",
    "\n",
    "We define the probability of each category as the prior probability which means how probable is it for a news to be for a certain category.\n",
    "\n",
    "We define the likelihood as the probability of each word of a news given the category which means how probable it is for that category to use that word. in other words, the likelihood probability is:\n",
    "\n",
    "$$ likelihood = {P(x_0, x_1, x_2, ...,  x_n|category)} $$\n",
    "\n",
    "Since the probability of existing a word in a certain category is independent of the probability of existing another word in that category for each news, we can multiply these probabilities to calculate our conditional probability.\n",
    "\n",
    "$$ likelihood = {P(x_0|category)\\times P(x_1|category) \\times P(x_2|category) \\times ... \\times P(x_n|category)} $$\n",
    "\n",
    "The evidence is the probabiliy of all words that we have in a given news.\n",
    "\n",
    "$$ evidence = {P(x_0, x_1, x_2, ...,  x_n)} $$\n",
    "\n",
    "Since we are going to compare the posterior probabilities for each category and in each category the evidence probability is the same as other categories, thus we don't need to calculate the evidence probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part $\\mathrm{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to train our model for only two categories. TRAVEL category and BUSINESS category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelWords = trainTravel.words.values\n",
    "travelWords = list(itertools.chain.from_iterable(travelWords))\n",
    "BusinessWords = trainBusiness.words.values\n",
    "BusinessWords = list(itertools.chain.from_iterable(BusinessWords))\n",
    "allTravelAndBusinessWord = list(map(''.join, set(itertools.chain(travelWords, BusinessWords))))\n",
    "travelWordsCount = dict(collections.Counter(travelWords))\n",
    "BusinessWordsCount = dict(collections.Counter(BusinessWords))\n",
    "newDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences'])\n",
    "for word in allTravelAndBusinessWord:\n",
    "    to = 0\n",
    "    bo = 0\n",
    "    if word in travelWordsCount:\n",
    "        to = travelWordsCount[word]\n",
    "    if word in BusinessWordsCount:\n",
    "        bo = BusinessWordsCount[word]\n",
    "    newDataFrame = newDataFrame.append({'Word' : word,'Travel occurrences' : to,'Business occurrences' : bo},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new data frame. for each word, we calculate the probability of that word given the category. in fact, we calculate $ P(word | category) $ for each word in our training set (in this case out training set contain only two mentioned categories).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the conditional probability we use the Laplace Smoothing. since we multiply the conditional probabilities in order to calculate the likelihood, if we have a word that used only one time in one category, the likelihood probability for other categories will be zero even if other conditional probabilities have a high value. Laplace smoothing is used to solve the problem of zero probability.\n",
    "\n",
    "$$ P(word|category) =  \\frac{O(word,category) + \\alpha}{S(words,categor) + |A|\\alpha} $$\n",
    "\n",
    "We use this formula to calculate the conditional probability. O(word, category) is the word occurrences in that category, S(words, category) is the number of all words in that category and |A| is the number of distinct words in our train set.\n",
    "alpha is a constant which is used to solve zero probability problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel occurrences</th>\n",
       "      <th>Business occurrences</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lighter</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.23313e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>join</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000227779</td>\n",
       "      <td>0.000416568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>família</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.23313e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bellavista</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.33988e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>luminous</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.23313e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>leary</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.33988e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fairness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.33988e-05</td>\n",
       "      <td>5.7176e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>whoever</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.12638e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>irkutsk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.33988e-05</td>\n",
       "      <td>8.168e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>july</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000200982</td>\n",
       "      <td>0.000269544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17833 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Travel occurrences Business occurrences Travel Probability  \\\n",
       "Word                                                                    \n",
       "lighter                     2                    0        2.23313e-05   \n",
       "join                       25                   25        0.000227779   \n",
       "família                     2                    0        2.23313e-05   \n",
       "bellavista                  1                    0        1.33988e-05   \n",
       "luminous                    2                    0        2.23313e-05   \n",
       "...                       ...                  ...                ...   \n",
       "leary                       1                    0        1.33988e-05   \n",
       "fairness                    1                    3        1.33988e-05   \n",
       "whoever                     3                    0        3.12638e-05   \n",
       "irkutsk                     1                    0        1.33988e-05   \n",
       "july                       22                   16        0.000200982   \n",
       "\n",
       "           Business Probability  \n",
       "Word                             \n",
       "lighter               8.168e-06  \n",
       "join                0.000416568  \n",
       "família               8.168e-06  \n",
       "bellavista            8.168e-06  \n",
       "luminous              8.168e-06  \n",
       "...                         ...  \n",
       "leary                 8.168e-06  \n",
       "fairness             5.7176e-05  \n",
       "whoever               8.168e-06  \n",
       "irkutsk               8.168e-06  \n",
       "july                0.000269544  \n",
       "\n",
       "[17833 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "newDataFrame['Travel Probability'] = (newDataFrame['Travel occurrences'] + alpha) / (newDataFrame['Travel occurrences'].sum() + (len(set(travelWords + BusinessWords))*alpha))\n",
    "newDataFrame['Business Probability'] = (newDataFrame['Business occurrences'] + alpha) / (newDataFrame['Business occurrences'].sum() + (len(set(travelWords + BusinessWords))*alpha))\n",
    "newDataFrame = newDataFrame.set_index('Word')\n",
    "newDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "travelAndBusinessEvaluateData = pd.concat([evaluateBusiness, evaluateTravel])\n",
    "travelAndBusinessEvaluateData = travelAndBusinessEvaluateData.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the conditional probability our model is ready to test with evaluate data. for each news in evaluate set, we calculate the prior probability and for all of the words in that news, we multiply the conditional probability with prior probability in order to calculate the posterior probability. After that, we can predict the category for each news. the category with higher posterior probability is our model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2452</td>\n",
       "      <td>2452</td>\n",
       "      <td>Peter Fuda, ContributorPrincipal, The Alignmen...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>Embed Routines and Rituals (Principle No. 5 of...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/embed-rou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>6.248244e-01</td>\n",
       "      <td>3.751756e-01</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>2130</td>\n",
       "      <td>24/7 Wall St., 24/7 Wall St.</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>10 Banks Foreclosing on the Most Homes: 24/7 W...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/bank-fore...</td>\n",
       "      <td>Despite not being directly on the hook for the...</td>\n",
       "      <td>[despite, directly, hook, majority, losses, in...</td>\n",
       "      <td>4.855343e-42</td>\n",
       "      <td>4.853664e-38</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12749</td>\n",
       "      <td>12749</td>\n",
       "      <td>Jamie Feldman</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>The Blooming Cherry Blossoms In Japan Look Dow...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/cherry-bl...</td>\n",
       "      <td>Time to book a flight.</td>\n",
       "      <td>[time, book, flight]</td>\n",
       "      <td>5.783270e-09</td>\n",
       "      <td>5.695067e-11</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3182</td>\n",
       "      <td>3182</td>\n",
       "      <td>Christopher Elliott, Contributor\\nAuthor, How ...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-04-25</td>\n",
       "      <td>Could Your Son be Next?</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/tsa-secur...</td>\n",
       "      <td>Do we really have to trade our dignity for sec...</td>\n",
       "      <td>[really, trade, dignity, security, tsa, trauma...</td>\n",
       "      <td>7.043141e-53</td>\n",
       "      <td>2.032097e-57</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17058</td>\n",
       "      <td>17058</td>\n",
       "      <td>Shelley Miller, Contributor\\nHome Exchange Expert</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>A Virtually Free Vacation Can Be Yours When Yo...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/a-virtual...</td>\n",
       "      <td>Suppose I told you that you can afford a vacat...</td>\n",
       "      <td>[suppose, tell, afford, vacation, even, travel...</td>\n",
       "      <td>3.364800e-52</td>\n",
       "      <td>2.051141e-60</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  category  \\\n",
       "2452    2452  Peter Fuda, ContributorPrincipal, The Alignmen...  BUSINESS   \n",
       "2130    2130                       24/7 Wall St., 24/7 Wall St.  BUSINESS   \n",
       "12749  12749                                      Jamie Feldman    TRAVEL   \n",
       "3182    3182  Christopher Elliott, Contributor\\nAuthor, How ...    TRAVEL   \n",
       "17058  17058  Shelley Miller, Contributor\\nHome Exchange Expert    TRAVEL   \n",
       "\n",
       "             date                                           headline  \\\n",
       "2452   2014-07-07  Embed Routines and Rituals (Principle No. 5 of...   \n",
       "2130   2013-03-23  10 Banks Foreclosing on the Most Homes: 24/7 W...   \n",
       "12749  2016-03-22  The Blooming Cherry Blossoms In Japan Look Dow...   \n",
       "3182   2013-04-25                            Could Your Son be Next?   \n",
       "17058  2012-04-27  A Virtually Free Vacation Can Be Yours When Yo...   \n",
       "\n",
       "                                                    link  \\\n",
       "2452   https://www.huffingtonpost.com/entry/embed-rou...   \n",
       "2130   https://www.huffingtonpost.com/entry/bank-fore...   \n",
       "12749  https://www.huffingtonpost.com/entry/cherry-bl...   \n",
       "3182   https://www.huffingtonpost.com/entry/tsa-secur...   \n",
       "17058  https://www.huffingtonpost.com/entry/a-virtual...   \n",
       "\n",
       "                                       short_description  \\\n",
       "2452                                                 NaN   \n",
       "2130   Despite not being directly on the hook for the...   \n",
       "12749                             Time to book a flight.   \n",
       "3182   Do we really have to trade our dignity for sec...   \n",
       "17058  Suppose I told you that you can afford a vacat...   \n",
       "\n",
       "                                                   words  Travel Probability  \\\n",
       "2452                                                  []        6.248244e-01   \n",
       "2130   [despite, directly, hook, majority, losses, in...        4.855343e-42   \n",
       "12749                               [time, book, flight]        5.783270e-09   \n",
       "3182   [really, trade, dignity, security, tsa, trauma...        7.043141e-53   \n",
       "17058  [suppose, tell, afford, vacation, even, travel...        3.364800e-52   \n",
       "\n",
       "       Business Probability Prediction  \n",
       "2452           3.751756e-01     TRAVEL  \n",
       "2130           4.853664e-38   BUSINESS  \n",
       "12749          5.695067e-11     TRAVEL  \n",
       "3182           2.032097e-57     TRAVEL  \n",
       "17058          2.051141e-60     TRAVEL  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,row in travelAndBusinessEvaluateData.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTravelAndBusinessWord:\n",
    "            travelPriorProbability *= newDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= newDataFrame.at[word,'Business Probability']\n",
    "\n",
    "    travelAndBusinessEvaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    travelAndBusinessEvaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability:\n",
    "        travelAndBusinessEvaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "    else:\n",
    "        travelAndBusinessEvaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "travelAndBusinessEvaluateData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8691011235955056 0.8082549634273772\n",
      "0.7380729653882133 0.8438502673796792\n",
      "0.8504738504738505\n"
     ]
    }
   ],
   "source": [
    "#Travel\n",
    "correctTravel = travelAndBusinessEvaluateData.loc[(travelAndBusinessEvaluateData['category'] == 'TRAVEL') & (travelAndBusinessEvaluateData['Prediction'] == 'TRAVEL')]\n",
    "correctTravel = (correctTravel.all(axis='columns')).sum()\n",
    "allOfTravel = (travelAndBusinessEvaluateData['category'] == 'TRAVEL').sum()\n",
    "allTravelPrediction = (travelAndBusinessEvaluateData['Prediction'] == 'TRAVEL').sum()\n",
    "travelRecall = correctTravel/allOfTravel\n",
    "travelPrecision = correctTravel/allTravelPrediction\n",
    "print(travelRecall,travelPrecision)\n",
    "#Business\n",
    "correctBusiness = travelAndBusinessEvaluateData.loc[(travelAndBusinessEvaluateData['category'] == 'BUSINESS') & (travelAndBusinessEvaluateData['Prediction'] == 'BUSINESS')]\n",
    "correctBusiness = (correctBusiness.all(axis='columns')).sum()\n",
    "allOfBusiness = (travelAndBusinessEvaluateData['category'] == 'BUSINESS').sum()\n",
    "allBusinessPrediction = (travelAndBusinessEvaluateData['Prediction'] == 'BUSINESS').sum()\n",
    "BusinessRecall = correctBusiness/allOfBusiness\n",
    "BusinessPrecision = correctBusiness/allBusinessPrediction\n",
    "print(BusinessRecall,BusinessPrecision)\n",
    "travelAndBusinessEvaluateData['correct'] = (travelAndBusinessEvaluateData['category'] == travelAndBusinessEvaluateData['Prediction'])\n",
    "correctDetected = (travelAndBusinessEvaluateData['correct']).sum()\n",
    "accuracy =  correctDetected / len(travelAndBusinessEvaluateData)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| phase1 | Travel  | Business  |\n",
    "| --- | --- | --- |\n",
    "| Recall |\n",
    "| Precision |\n",
    "| Accuracy   |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part $\\mathrm{II}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add third category to our model. we add STYLE & BEAUTY category to our train set and repeat all the process again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103034 52298 86274 22282\n",
      "14426 8749 11301 22282\n"
     ]
    }
   ],
   "source": [
    "sbWords = trainSB.words.values\n",
    "sbWords = list(itertools.chain.from_iterable(sbWords))\n",
    "allTrainDataWords = list(map(''.join, set(itertools.chain(travelWords, BusinessWords,sbWords))))\n",
    "print(len(travelWords),len(BusinessWords),len(sbWords),len(allTrainDataWords))\n",
    "sbWordsCount = dict(collections.Counter(sbWords))\n",
    "travelWordsCount = dict(collections.Counter(travelWords))\n",
    "BusinessWordsCount = dict(collections.Counter(BusinessWords))\n",
    "print(len(travelWordsCount),len(BusinessWordsCount),len(sbWordsCount),len(allTrainDataWords))\n",
    "allDataFrame = pd.DataFrame(columns=['Word','Travel occurrences','Business occurrences','Style & Beauty occurrences'])\n",
    "for word in allTrainDataWords:\n",
    "    to = 0\n",
    "    sbo = 0\n",
    "    bo = 0\n",
    "    if word in travelWordsCount:\n",
    "        to = travelWordsCount[word]\n",
    "    if word in BusinessWordsCount:\n",
    "        bo = BusinessWordsCount[word]\n",
    "    if word in sbWordsCount:\n",
    "        sbo = sbWordsCount[word]\n",
    "    allDataFrame = allDataFrame.append({'Word' : word,'Travel occurrences' : to,'Business occurrences' : bo,'Style & Beauty occurrences' : sbo},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use Laplace smoothing here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Travel occurrences</th>\n",
       "      <th>Business occurrences</th>\n",
       "      <th>Style &amp; Beauty occurrences</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>luminous</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.18962e-05</td>\n",
       "      <td>7.88159e-06</td>\n",
       "      <td>4.61941e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>refinancers</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.37924e-06</td>\n",
       "      <td>3.94079e-05</td>\n",
       "      <td>5.13268e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tack</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.06547e-05</td>\n",
       "      <td>7.88159e-06</td>\n",
       "      <td>1.5398e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>reload</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37924e-06</td>\n",
       "      <td>7.88159e-06</td>\n",
       "      <td>1.5398e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>puzzle</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.94132e-05</td>\n",
       "      <td>7.88159e-06</td>\n",
       "      <td>1.5398e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Travel occurrences Business occurrences  \\\n",
       "Word                                                  \n",
       "luminous                     2                    0   \n",
       "refinancers                  0                    2   \n",
       "tack                         3                    0   \n",
       "reload                       0                    0   \n",
       "puzzle                       4                    0   \n",
       "\n",
       "            Style & Beauty occurrences Travel Probability  \\\n",
       "Word                                                        \n",
       "luminous                             4        2.18962e-05   \n",
       "refinancers                          0        4.37924e-06   \n",
       "tack                                 1        3.06547e-05   \n",
       "reload                               1        4.37924e-06   \n",
       "puzzle                               1        3.94132e-05   \n",
       "\n",
       "            Business Probability Style & Beauty Probability  \n",
       "Word                                                         \n",
       "luminous             7.88159e-06                4.61941e-05  \n",
       "refinancers          3.94079e-05                5.13268e-06  \n",
       "tack                 7.88159e-06                 1.5398e-05  \n",
       "reload               7.88159e-06                 1.5398e-05  \n",
       "puzzle               7.88159e-06                 1.5398e-05  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "allDataFrame['Travel Probability'] = (allDataFrame['Travel occurrences'] + alpha) / (allDataFrame['Travel occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Business Probability'] = (allDataFrame['Business occurrences'] + alpha) / (allDataFrame['Business occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame['Style & Beauty Probability'] = (allDataFrame['Style & Beauty occurrences'] + alpha) / (allDataFrame['Style & Beauty occurrences'].sum() + (len(set(travelWords + BusinessWords + sbWords))*alpha))\n",
    "allDataFrame = allDataFrame.set_index('Word')\n",
    "allDataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model with our evaluation set which has all three category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in evaluateData.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    sbPriorProbability = len(trainSB)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTrainDataWords:\n",
    "            travelPriorProbability *= allDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= allDataFrame.at[word,'Business Probability']\n",
    "            sbPriorProbability *= allDataFrame.at[word,'Style & Beauty Probability']\n",
    "    evaluateData.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    evaluateData.at[index,'Business Probability'] = businessPriorProbability\n",
    "    evaluateData.at[index,'Style & Beauty Probability'] = sbPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability and travelPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'TRAVEL'\n",
    "    if businessPriorProbability >= travelPriorProbability and businessPriorProbability >= sbPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'BUSINESS'\n",
    "    if sbPriorProbability >= travelPriorProbability and sbPriorProbability>= businessPriorProbability:\n",
    "        evaluateData.at[index,'Prediction'] = 'STYLE & BEAUTY'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>words</th>\n",
       "      <th>Travel Probability</th>\n",
       "      <th>Business Probability</th>\n",
       "      <th>Style &amp; Beauty Probability</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5492</td>\n",
       "      <td>5492</td>\n",
       "      <td>Amanda McGowan</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>Iman Looking Fierce, Long Before David Bowie (...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/iman-davi...</td>\n",
       "      <td>Apparently not.</td>\n",
       "      <td>[apparently]</td>\n",
       "      <td>5.609127e-05</td>\n",
       "      <td>3.122637e-05</td>\n",
       "      <td>5.249641e-05</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2211</td>\n",
       "      <td>2211</td>\n",
       "      <td>Catherine Garvin, Contributor\\nWriter, Author,...</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2014-03-14</td>\n",
       "      <td>Shear Madness TV Personality, Natalie Redding ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/shear-mad...</td>\n",
       "      <td>Boho chic Natalie wears white linen pants by J...</td>\n",
       "      <td>[boho, chic, natalie, wear, white, linen, pant...</td>\n",
       "      <td>2.260183e-72</td>\n",
       "      <td>4.416763e-74</td>\n",
       "      <td>1.555949e-64</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15312</td>\n",
       "      <td>15312</td>\n",
       "      <td>Renee Jacques</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>Why Shopping Malls Really Aren't Fun At All (P...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/shopping-...</td>\n",
       "      <td>1. The place is just WAY too big. You basicall...</td>\n",
       "      <td>[place, way, big, basically, workout, day, go,...</td>\n",
       "      <td>2.453837e-37</td>\n",
       "      <td>2.679207e-38</td>\n",
       "      <td>2.256868e-37</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>1956</td>\n",
       "      <td>Meg Waite Clayton, Contributor\\nbestselling au...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>6 Weeks in the English Lakes #3: Beatrix Potte...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/6-weeks-i...</td>\n",
       "      <td>Many people come to the English Lakes to immer...</td>\n",
       "      <td>[many, people, come, english, lakes, immerse, ...</td>\n",
       "      <td>3.579503e-67</td>\n",
       "      <td>2.000793e-73</td>\n",
       "      <td>1.524785e-75</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21266</td>\n",
       "      <td>21266</td>\n",
       "      <td>Daniel Burrus, ContributorTechnology Futurist,...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>Profiting From the Speed of Change</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/profiting...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>3.881346e-01</td>\n",
       "      <td>2.330552e-01</td>\n",
       "      <td>3.788102e-01</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            authors  \\\n",
       "5492    5492                                     Amanda McGowan   \n",
       "2211    2211  Catherine Garvin, Contributor\\nWriter, Author,...   \n",
       "15312  15312                                      Renee Jacques   \n",
       "1956    1956  Meg Waite Clayton, Contributor\\nbestselling au...   \n",
       "21266  21266  Daniel Burrus, ContributorTechnology Futurist,...   \n",
       "\n",
       "             category        date  \\\n",
       "5492   STYLE & BEAUTY  2012-11-20   \n",
       "2211   STYLE & BEAUTY  2014-03-14   \n",
       "15312  STYLE & BEAUTY  2013-06-25   \n",
       "1956           TRAVEL  2013-06-25   \n",
       "21266        BUSINESS  2014-10-09   \n",
       "\n",
       "                                                headline  \\\n",
       "5492   Iman Looking Fierce, Long Before David Bowie (...   \n",
       "2211   Shear Madness TV Personality, Natalie Redding ...   \n",
       "15312  Why Shopping Malls Really Aren't Fun At All (P...   \n",
       "1956   6 Weeks in the English Lakes #3: Beatrix Potte...   \n",
       "21266                 Profiting From the Speed of Change   \n",
       "\n",
       "                                                    link  \\\n",
       "5492   https://www.huffingtonpost.com/entry/iman-davi...   \n",
       "2211   https://www.huffingtonpost.com/entry/shear-mad...   \n",
       "15312  https://www.huffingtonpost.com/entry/shopping-...   \n",
       "1956   https://www.huffingtonpost.com/entry/6-weeks-i...   \n",
       "21266  https://www.huffingtonpost.com/entry/profiting...   \n",
       "\n",
       "                                       short_description  \\\n",
       "5492                                     Apparently not.   \n",
       "2211   Boho chic Natalie wears white linen pants by J...   \n",
       "15312  1. The place is just WAY too big. You basicall...   \n",
       "1956   Many people come to the English Lakes to immer...   \n",
       "21266                                                NaN   \n",
       "\n",
       "                                                   words  Travel Probability  \\\n",
       "5492                                        [apparently]        5.609127e-05   \n",
       "2211   [boho, chic, natalie, wear, white, linen, pant...        2.260183e-72   \n",
       "15312  [place, way, big, basically, workout, day, go,...        2.453837e-37   \n",
       "1956   [many, people, come, english, lakes, immerse, ...        3.579503e-67   \n",
       "21266                                                 []        3.881346e-01   \n",
       "\n",
       "       Business Probability  Style & Beauty Probability      Prediction  \n",
       "5492           3.122637e-05                5.249641e-05          TRAVEL  \n",
       "2211           4.416763e-74                1.555949e-64  STYLE & BEAUTY  \n",
       "15312          2.679207e-38                2.256868e-37          TRAVEL  \n",
       "1956           2.000793e-73                1.524785e-75          TRAVEL  \n",
       "21266          2.330552e-01                3.788102e-01          TRAVEL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247191011235955 0.742914979757085\n",
      "0.7128157156220767 0.7650602409638554\n",
      "0.8480138169257341 0.912639405204461\n",
      "0.8264282599215003\n"
     ]
    }
   ],
   "source": [
    "#Travel\n",
    "correctTravel = evaluateData.loc[(evaluateData['category'] == 'TRAVEL') & (evaluateData['Prediction'] == 'TRAVEL')]\n",
    "correctTravel = (correctTravel.all(axis='columns')).sum()\n",
    "allOfTravel = (evaluateData['category'] == 'TRAVEL').sum()\n",
    "allTravelPrediction = (evaluateData['Prediction'] == 'TRAVEL').sum()\n",
    "travelRecall = correctTravel/allOfTravel\n",
    "travelPrecision = correctTravel/allTravelPrediction\n",
    "print(travelRecall,travelPrecision)\n",
    "#Business\n",
    "correctBusiness = evaluateData.loc[(evaluateData['category'] == 'BUSINESS') & (evaluateData['Prediction'] == 'BUSINESS')]\n",
    "correctBusiness = (correctBusiness.all(axis='columns')).sum()\n",
    "allOfBusiness = (evaluateData['category'] == 'BUSINESS').sum()\n",
    "allBusinessPrediction = (evaluateData['Prediction'] == 'BUSINESS').sum()\n",
    "BusinessRecall = correctBusiness/allOfBusiness\n",
    "BusinessPrecision = correctBusiness/allBusinessPrediction\n",
    "print(BusinessRecall,BusinessPrecision)\n",
    "#Style\n",
    "correctSB = evaluateData.loc[(evaluateData['category'] == 'STYLE & BEAUTY') & (evaluateData['Prediction'] == 'STYLE & BEAUTY')]\n",
    "correctSB = (correctSB.all(axis='columns')).sum()\n",
    "allOfSB = (evaluateData['category'] == 'STYLE & BEAUTY').sum()\n",
    "allSBPrediction = (evaluateData['Prediction'] == 'STYLE & BEAUTY').sum()\n",
    "SBRecall = correctSB/allOfSB\n",
    "SBPrecision = correctSB/allSBPrediction\n",
    "print(SBRecall,SBPrecision)\n",
    "\n",
    "evaluateData['correct'] = (evaluateData['category'] == evaluateData['Prediction'])\n",
    "correctDetected = (evaluateData['correct']).sum()\n",
    "accuracy =  correctDetected / len(evaluateData)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| phase1 | Travel  | Business | Style & Beauty |\n",
    "| --- | --- | --- | --- |\n",
    "| Recall |\n",
    "| Precision |\n",
    "| Accuracy   |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset (test.csv) which is similar to our train dataset but has no category. we want to predict the category of news for this dataset with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataFrame = pd.read_csv('Attachment/test.csv')\n",
    "testDataFrame = cleanText(testDataFrame)\n",
    "testDataFrame = testDataFrame.dropna(subset=['short_description'])\n",
    "for index,row in testDataFrame.iterrows():\n",
    "    travelPriorProbability = len(trainTravel)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    businessPriorProbability = len(trainBusiness)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    sbPriorProbability = len(trainSB)/(len(trainTravel)+len(trainBusiness)+len(trainSB))\n",
    "    words = set(row['words'])\n",
    "    for word in words:\n",
    "        if word in allTrainDataWords:\n",
    "            travelPriorProbability *= allDataFrame.at[word,'Travel Probability']\n",
    "            businessPriorProbability *= allDataFrame.at[word,'Business Probability']\n",
    "            sbPriorProbability *= allDataFrame.at[word,'Style & Beauty Probability']\n",
    "    testDataFrame.at[index,'Travel Probability'] = travelPriorProbability\n",
    "    testDataFrame.at[index,'Business Probability'] = businessPriorProbability\n",
    "    testDataFrame.at[index,'Style & Beauty Probability'] = sbPriorProbability\n",
    "    if travelPriorProbability >= businessPriorProbability and travelPriorProbability >= sbPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'TRAVEL'\n",
    "    if businessPriorProbability >= travelPriorProbability and businessPriorProbability >= sbPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'BUSINESS'\n",
    "    if sbPriorProbability >= travelPriorProbability and sbPriorProbability>= businessPriorProbability:\n",
    "        testDataFrame.at[index,'category'] = 'STYLE & BEAUTY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the category prediction into the ouptut.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2543</td>\n",
       "      <td>2543</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2544</td>\n",
       "      <td>2544</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2545</td>\n",
       "      <td>2545</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2546</td>\n",
       "      <td>2546</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2547</td>\n",
       "      <td>2547</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        category\n",
       "0         0  STYLE & BEAUTY\n",
       "1         1          TRAVEL\n",
       "4         4  STYLE & BEAUTY\n",
       "5         5          TRAVEL\n",
       "6         6  STYLE & BEAUTY\n",
       "...     ...             ...\n",
       "2543   2543        BUSINESS\n",
       "2544   2544          TRAVEL\n",
       "2545   2545        BUSINESS\n",
       "2546   2546          TRAVEL\n",
       "2547   2547        BUSINESS\n",
       "\n",
       "[2419 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({\"index\": testDataFrame['index'],\"category\": testDataFrame['category']})\n",
    "output.to_csv('output.csv', index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. On the other hand, Lemmatization reduces the inflected words properly ensuring that the root word belongs to the language. The result of Lemmatization is called lemma which is dictionary form, or citation form of the words. \n",
    "We use Lemmatization in our project because we interested in word frequency and want to treat all forms of a word as one. in stemming some forms of a word may not have the same stem and that can influence the frequency of that word and the accuracy of our model as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf is short for \"term frequency-inverse document frequency\" which basically reflects how important a word is to a document.\n",
    "tf measures how frequently a term occurs in a document.\n",
    "\n",
    "TF = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "idf measures how important a term is.\n",
    "\n",
    "IDF = $log_e$(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "In Bayesian model, when we calculate the probabilities based on each word occurrences, each word in each document (in our case each news) counted as one. if we want to use tf-idf, instead of counting each word as one, we use the tf-idf weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a hundred percent precision for certain a category even if our model cannot predict the categories correctly. for example, if we predict one TRAVEL news correctly and then assign all other news to other categories (e.g BUSINESS) then the precision of the TRAVEL will be a hundred percent although our predictions are wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We referred to this problem earlier and the Laplace smoothing as a solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Mehrdad': virtualenv)",
   "language": "python",
   "name": "python37464bitmehrdadvirtualenv5fa8cbb105824cae8f34d6b894d7a675"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
